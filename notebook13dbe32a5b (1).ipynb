{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-20T16:10:30.170335Z","iopub.execute_input":"2023-05-20T16:10:30.172369Z","iopub.status.idle":"2023-05-20T16:10:30.204479Z","shell.execute_reply.started":"2023-05-20T16:10:30.172321Z","shell.execute_reply":"2023-05-20T16:10:30.201995Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:10:30.205848Z","iopub.execute_input":"2023-05-20T16:10:30.206895Z","iopub.status.idle":"2023-05-20T16:10:31.270368Z","shell.execute_reply.started":"2023-05-20T16:10:30.206864Z","shell.execute_reply":"2023-05-20T16:10:31.269128Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\narxiv_data = []\nfor line in open('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'):\n        arxiv_data.append(json.loads(line))\n    \nprint(len(arxiv_data))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:26:31.922374Z","iopub.execute_input":"2023-05-21T04:26:31.922797Z","iopub.status.idle":"2023-05-21T04:28:16.816271Z","shell.execute_reply.started":"2023-05-21T04:26:31.922760Z","shell.execute_reply":"2023-05-21T04:28:16.815138Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2258347\n","output_type":"stream"}]},{"cell_type":"code","source":"cs_data = []\ncv_data = []\ncategories = [\"cs.AI\", \"cs.DS\", \"cs.LG\", \"cs.NE\", \"cs.RO\", \"cs.CL\", \"cs.CV\"]\nfor t in arxiv_data:\n    if t['categories'] in categories:\n        cs_data.append(t)\n    if t['categories'] == 'cs.CV':\n        cv_data.append(t)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:28:16.818280Z","iopub.execute_input":"2023-05-21T04:28:16.818618Z","iopub.status.idle":"2023-05-21T04:28:18.796606Z","shell.execute_reply.started":"2023-05-21T04:28:16.818587Z","shell.execute_reply":"2023-05-21T04:28:18.795528Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"len(cs_data), len(cv_data)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:28:18.798324Z","iopub.execute_input":"2023-05-21T04:28:18.799060Z","iopub.status.idle":"2023-05-21T04:28:18.808781Z","shell.execute_reply.started":"2023-05-21T04:28:18.799015Z","shell.execute_reply":"2023-05-21T04:28:18.807426Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(103272, 45588)"},"metadata":{}}]},{"cell_type":"code","source":"cs_data[3]","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:12:37.668680Z","iopub.execute_input":"2023-05-20T16:12:37.669690Z","iopub.status.idle":"2023-05-20T16:12:37.681481Z","shell.execute_reply.started":"2023-05-20T16:12:37.669651Z","shell.execute_reply":"2023-05-20T16:12:37.680371Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'id': '0704.1274',\n 'submitter': 'Dev Rajnarayan',\n 'authors': 'David H. Wolpert and Dev G. Rajnarayan',\n 'title': 'Parametric Learning and Monte Carlo Optimization',\n 'comments': None,\n 'journal-ref': None,\n 'doi': None,\n 'report-no': None,\n 'categories': 'cs.LG',\n 'license': None,\n 'abstract': \"  This paper uncovers and explores the close relationship between Monte Carlo\\nOptimization of a parametrized integral (MCO), Parametric machine-Learning\\n(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\\ncontributions. First, we prove that MCO is mathematically identical to a broad\\nclass of PL problems. This identity potentially provides a new application\\ndomain for all broadly applicable PL techniques: MCO. Second, we introduce\\nimmediate sampling, a new version of the Probability Collectives (PC) algorithm\\nfor blackbox optimization. Immediate sampling transforms the original BO\\nproblem into an MCO problem. Accordingly, by combining these first two\\ncontributions, we can apply all PL techniques to BO. In our third contribution\\nwe validate this way of improving BO by demonstrating that cross-validation and\\nbagging improve immediate sampling. Finally, conventional MC and MCO procedures\\nignore the relationship between the sample point locations and the associated\\nvalues of the integrand; only the values of the integrand at those locations\\nare considered. We demonstrate that one can exploit the sample location\\ninformation using PL techniques, for example by forming a fit of the sample\\nlocations to the associated values of the integrand. This provides an\\nadditional way to apply PL techniques to improve MCO.\\n\",\n 'versions': [{'version': 'v1', 'created': 'Tue, 10 Apr 2007 17:01:07 GMT'}],\n 'update_date': '2011-11-09',\n 'authors_parsed': [['Wolpert', 'David H.', ''], ['Rajnarayan', 'Dev G.', '']]}"},"metadata":{}}]},{"cell_type":"code","source":"cv_data[3]","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:12:37.683197Z","iopub.execute_input":"2023-05-20T16:12:37.683539Z","iopub.status.idle":"2023-05-20T16:12:37.695338Z","shell.execute_reply.started":"2023-05-20T16:12:37.683508Z","shell.execute_reply":"2023-05-20T16:12:37.694286Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'id': '0705.0781',\n 'submitter': 'Tshilidzi Marwala',\n 'authors': 'Jonathan M.Spiller and T. Marwala',\n 'title': 'Medical Image Segmentation and Localization using Deformable Templates',\n 'comments': '4 pages',\n 'journal-ref': None,\n 'doi': None,\n 'report-no': None,\n 'categories': 'cs.CV',\n 'license': None,\n 'abstract': '  This paper presents deformable templates as a tool for segmentation and\\nlocalization of biological structures in medical images. Structures are\\nrepresented by a prototype template, combined with a parametric warp mapping\\nused to deform the original shape. The localization procedure is achieved using\\na multi-stage, multi-resolution algorithm de-signed to reduce computational\\ncomplexity and time. The algorithm initially identifies regions in the image\\nmost likely to contain the desired objects and then examines these regions at\\nprogressively increasing resolutions. The final stage of the algorithm involves\\nwarping the prototype template to match the localized objects. The algorithm is\\npresented along with the results of four example applications using MRI, x-ray\\nand ultrasound images.\\n',\n 'versions': [{'version': 'v1', 'created': 'Sun, 6 May 2007 06:02:46 GMT'}],\n 'update_date': '2007-05-23',\n 'authors_parsed': [['Spiller', 'Jonathan M.', ''], ['Marwala', 'T.', '']]}"},"metadata":{}}]},{"cell_type":"code","source":"import jieba\nfrom tqdm import tqdm\nstop_words = ['too', 'himself', \"wasn't\", 'by', 'yourself', 'aren', 'or', 'this',\n              'i', \"should've\", \"you'd\", 'mightn', 'if', 'yourselves', 'to', 'over',\n              'nor', 'itself', 'ours', 'down', 'd', 'be', 'up', 'any', \n              'such', 'when', 'm', \"you've\", 'been', 'more', 'about', 'while', \n              'ma', 'my', \"mightn't\", \"haven't\", 'is', 'there', 'how', 'few', \n              \"she's\", 'a', 'from', 'why', 'same', 'just', 'they', 'should',\n              'and', 'it', 'has', 'wouldn', 'further', \"couldn't\", 'those', 've',\n              'shan', 'our', 'once', 'ain', 'but', 'out', \"hadn't\", 'hadn', \n              'now', 'of', \"won't\", 'on', \"shan't\", 'than', 'had', 'am', 'his',\n              'above', \"aren't\", 'have', 'the', 'because', 'hasn', 'at', 'between', \n              'with', 'yours', \"that'll\", 'who', 'weren', \"doesn't\", \"mustn't\", \n              'before', 'wasn', 'under', 'its', 'below', 'her', 'hers', 'do', 'after', \n              'him', \"don't\", 'isn', 'having', 'll', 'no', 'until', 'very', 'which',\n              'these', 'myself', 'in', 'won', 'whom', 'me', 'can', 'themselves', 'didn',\n              'did', 'again', 'each', 'both', 'own', \"weren't\", 'you', 'what', 's',\n              'are', 'she', 'as', 'for', 'all', \"wouldn't\", 'against', \"didn't\", 'not', \n              'then', 'he', 'does', 'during', \"isn't\", 'o', 't', 'off', 'haven', 'other',\n              'so', 'most', 'that', \"hasn't\", 'being', 'through', 'some', 'mustn', \"shouldn't\",\n              'an', 'don', 'herself', 'shouldn', 'will', 'where', 'were', 'here', 'only',\n              'ourselves', 're', 'doesn', \"it's\", \"you're\", 'we', 'doing', 'your', 'needn',\n              \"you'll\", \"needn't\", 'them', 'into', 'theirs', 'y', 'couldn', 'their', 'was', ' ',\n              '`', '\\'', ':', ';', '\\n', '(', ')', '{', '}', '.', ',', '-', 'one', 'two']\ndef has_no_digit(string):\n    return not any(ch.isdigit() for ch in string)\ntotal_word = []\nfor i in tqdm(range(len(cs_data))):\n    text = cs_data[i]['abstract']\n    word = jieba.lcut(text, cut_all=False)\n    word_filter = [w for w in word if w.lower() not in stop_words and len(w)>=3 and has_no_digit(w)]\n    total_word.append(word_filter)\n\ntotal_word_cv = []\nfor i in tqdm(range(len(cv_data))):\n    text = cv_data[i]['abstract']\n    word = jieba.lcut(text, cut_all=False)\n    word_filter = [w for w in word if w.lower() not in stop_words and len(w)>=3 and has_no_digit(w)]\n    total_word_cv.append(word_filter)\nlen(total_word_cv)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:28:18.812766Z","iopub.execute_input":"2023-05-21T04:28:18.813856Z","iopub.status.idle":"2023-05-21T04:44:10.444316Z","shell.execute_reply.started":"2023-05-21T04:28:18.813819Z","shell.execute_reply":"2023-05-21T04:44:10.443204Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"  0%|          | 0/103272 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 1.362 seconds.\nPrefix dict has been built successfully.\n100%|██████████| 103272/103272 [10:48<00:00, 159.32it/s]\n100%|██████████| 45588/45588 [05:03<00:00, 150.30it/s]\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"45588"},"metadata":{}}]},{"cell_type":"code","source":"with open('/kaggle/working/total_words.txt', 'w') as f:\n    f.write(str(total_word))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:44:10.446286Z","iopub.execute_input":"2023-05-21T04:44:10.446723Z","iopub.status.idle":"2023-05-21T04:44:12.264001Z","shell.execute_reply.started":"2023-05-21T04:44:10.446682Z","shell.execute_reply":"2023-05-21T04:44:12.263055Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/total_words_cv.txt', 'w') as f:\n    f.write(str(total_word_cv))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:44:12.265425Z","iopub.execute_input":"2023-05-21T04:44:12.265751Z","iopub.status.idle":"2023-05-21T04:44:13.084550Z","shell.execute_reply.started":"2023-05-21T04:44:12.265722Z","shell.execute_reply":"2023-05-21T04:44:13.083489Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nvectorizer = TfidfVectorizer()\ntotal_sentence = [' '.join(l) for l in total_word]\ntfidf_matrix = vectorizer.fit_transform(total_sentence)\nprint(tfidf_matrix.shape)\nword_list = vectorizer.get_feature_names_out()\ntop_k_num = 20","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:44:13.086361Z","iopub.execute_input":"2023-05-21T04:44:13.086776Z","iopub.status.idle":"2023-05-21T04:44:28.998543Z","shell.execute_reply.started":"2023-05-21T04:44:13.086734Z","shell.execute_reply":"2023-05-21T04:44:28.997063Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"(103272, 101149)\n","output_type":"stream"}]},{"cell_type":"code","source":"word_list = np.array(word_list)\ntop_list = []\nfor i in tqdm(range(tfidf_matrix.shape[0])):\n    m = tfidf_matrix[i]\n    top_idx = np.argsort(m.data)[::-1][:top_k_num+1]\n    top_list.append(word_list[m.indices[top_idx]])","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:44:29.000009Z","iopub.execute_input":"2023-05-21T04:44:29.000917Z","iopub.status.idle":"2023-05-21T04:44:41.787211Z","shell.execute_reply.started":"2023-05-21T04:44:29.000863Z","shell.execute_reply":"2023-05-21T04:44:41.786077Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 103272/103272 [00:12<00:00, 8084.83it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"total_num = 0\ncnt_dict = {}\nfor l in tqdm(total_word):\n    for word in l:\n        total_num += 1\n        if word not in cnt_dict.keys():\n            cnt_dict[word] = 1\n        else:\n            cnt_dict[word] += 1\ncnt_dict = dict(sorted(cnt_dict.items(), key=lambda x: x[1], reverse=True))\ncnt_dict, total_num","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:44:41.788728Z","iopub.execute_input":"2023-05-21T04:44:41.789171Z","iopub.status.idle":"2023-05-21T04:44:49.188271Z","shell.execute_reply.started":"2023-05-21T04:44:41.789127Z","shell.execute_reply":"2023-05-21T04:44:49.187283Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 103272/103272 [00:07<00:00, 14267.97it/s]\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"({'model': 74643,\n  'based': 69367,\n  'data': 69307,\n  'learning': 64675,\n  'method': 54748,\n  'paper': 50636,\n  'image': 49404,\n  'models': 49311,\n  'methods': 48991,\n  'performance': 48204,\n  'propose': 46984,\n  'proposed': 46164,\n  'results': 43631,\n  'problem': 43420,\n  'show': 42345,\n  'using': 41461,\n  'training': 40241,\n  'approach': 39199,\n  'time': 38628,\n  'state': 38341,\n  'network': 37344,\n  'algorithm': 36388,\n  'images': 34741,\n  'information': 33992,\n  'task': 33964,\n  'different': 33420,\n  'art': 32951,\n  'new': 30553,\n  'also': 30170,\n  'features': 29685,\n  'datasets': 29419,\n  'dataset': 29091,\n  'tasks': 28407,\n  'novel': 27442,\n  'used': 27349,\n  'work': 27099,\n  'detection': 26368,\n  'neural': 26097,\n  'framework': 25999,\n  'object': 24806,\n  'However': 24243,\n  'large': 23961,\n  'deep': 23701,\n  'real': 23635,\n  'high': 22973,\n  'algorithms': 22760,\n  'present': 22720,\n  'use': 22691,\n  'demonstrate': 22222,\n  'feature': 22079,\n  'networks': 21975,\n  'multi': 21867,\n  'set': 21590,\n  'domain': 21319,\n  'human': 21220,\n  'accuracy': 21182,\n  'experiments': 21132,\n  'classification': 20969,\n  'system': 20889,\n  'language': 20563,\n  'existing': 20454,\n  'well': 20338,\n  'segmentation': 20269,\n  'first': 20264,\n  'level': 20148,\n  'graph': 18634,\n  'trained': 18515,\n  'knowledge': 18040,\n  'space': 17916,\n  'approaches': 17506,\n  'semantic': 17108,\n  'text': 17050,\n  'recognition': 16940,\n  'end': 16829,\n  'number': 16743,\n  'available': 16730,\n  'representation': 16270,\n  'video': 15990,\n  'attention': 15946,\n  'problems': 15809,\n  'analysis': 15590,\n  'multiple': 15561,\n  'input': 15340,\n  'systems': 15304,\n  'better': 14965,\n  'scale': 14588,\n  'study': 14308,\n  'applications': 14288,\n  'single': 14173,\n  'visual': 14161,\n  'quality': 14130,\n  'many': 14034,\n  'target': 13709,\n  'supervised': 13613,\n  'improve': 13454,\n  'provide': 13315,\n  'introduce': 13257,\n  'learn': 13249,\n  'process': 13230,\n  'challenging': 13230,\n  'non': 13205,\n  'objects': 13114,\n  'three': 12921,\n  'world': 12915,\n  'various': 12899,\n  'without': 12896,\n  'research': 12877,\n  'point': 12815,\n  'several': 12804,\n  'low': 12669,\n  'local': 12667,\n  'design': 12635,\n  'structure': 12380,\n  'efficient': 12187,\n  'representations': 12177,\n  'achieve': 12070,\n  'loss': 11987,\n  'given': 11982,\n  'prediction': 11932,\n  'order': 11593,\n  'class': 11557,\n  'estimation': 11494,\n  'compared': 11489,\n  'achieves': 11479,\n  'robot': 11462,\n  'outperforms': 11457,\n  'due': 11386,\n  'techniques': 11285,\n  'machine': 11217,\n  'function': 11028,\n  'optimization': 11018,\n  'temporal': 11009,\n  'previous': 10936,\n  'specific': 10925,\n  'motion': 10875,\n  'architecture': 10824,\n  'evaluation': 10722,\n  'pre': 10605,\n  'recent': 10601,\n  'important': 10528,\n  'context': 10443,\n  'pose': 10417,\n  'source': 10351,\n  'face': 10349,\n  'effective': 10345,\n  'benchmark': 10311,\n  'simple': 10266,\n  'best': 10249,\n  'address': 10179,\n  'significantly': 10096,\n  'parameters': 10068,\n  'distribution': 10020,\n  'search': 9991,\n  'generation': 9869,\n  'vision': 9868,\n  'including': 9665,\n  'depth': 9614,\n  'generate': 9494,\n  'may': 9478,\n  'self': 9459,\n  'effectiveness': 9414,\n  'often': 9349,\n  'optimal': 9326,\n  'convolutional': 9219,\n  'cost': 9173,\n  'https': 9173,\n  'natural': 9158,\n  'evaluate': 9116,\n  'fine': 9093,\n  'complex': 9081,\n  'samples': 9025,\n  'CNN': 9011,\n  'scene': 8977,\n  'test': 8975,\n  'robust': 8952,\n  'known': 8924,\n  'labels': 8851,\n  'across': 8842,\n  'size': 8776,\n  'general': 8740,\n  'solution': 8706,\n  'control': 8703,\n  'spatial': 8696,\n  'small': 8675,\n  'find': 8584,\n  'processing': 8579,\n  'perform': 8519,\n  'significant': 8470,\n  'word': 8454,\n  'global': 8432,\n  'current': 8396,\n  'github': 8356,\n  'even': 8352,\n  'dynamic': 8346,\n  'module': 8331,\n  'com': 8198,\n  'view': 8165,\n  'inference': 8163,\n  'linear': 8129,\n  'computational': 8118,\n  'generated': 8113,\n  'case': 8040,\n  'log': 8033,\n  'standard': 8026,\n  'action': 8016,\n  'matching': 7988,\n  'resolution': 7888,\n  'code': 7862,\n  'terms': 7854,\n  'train': 7853,\n  'translation': 7851,\n  'cross': 7849,\n  'sequence': 7833,\n  'called': 7827,\n  'strategy': 7772,\n  'learned': 7768,\n  'result': 7760,\n  'graphs': 7698,\n  'via': 7557,\n  'limited': 7466,\n  'complexity': 7414,\n  'applied': 7375,\n  'accurate': 7366,\n  'field': 7324,\n  'adversarial': 7314,\n  'long': 7312,\n  'tracking': 7311,\n  'camera': 7291,\n  'online': 7289,\n  'Specifically': 7252,\n  'within': 7228,\n  'way': 7204,\n  'key': 7199,\n  'works': 7190,\n  'allows': 7183,\n  'able': 7179,\n  'classes': 7177,\n  'particular': 7174,\n  'transfer': 7153,\n  'experimental': 7124,\n  'label': 7106,\n  'error': 7066,\n  'make': 7056,\n  'Finally': 7054,\n  'achieved': 6981,\n  'Moreover': 6968,\n  'like': 6967,\n  'computer': 6956,\n  'approximation': 6940,\n  'range': 6907,\n  'modeling': 6901,\n  'points': 6893,\n  'addition': 6882,\n  'designed': 6809,\n  'obtain': 6772,\n  'focus': 6766,\n  'domains': 6745,\n  'still': 6729,\n  'ability': 6721,\n  'prior': 6696,\n  'Furthermore': 6683,\n  'shown': 6682,\n  'fully': 6676,\n  'environment': 6664,\n  'map': 6664,\n  'Extensive': 6663,\n  'provides': 6649,\n  'step': 6646,\n  'technique': 6644,\n  'benchmarks': 6574,\n  'efficiency': 6536,\n  'layer': 6516,\n  'shape': 6491,\n  'challenge': 6470,\n  'scenarios': 6428,\n  'similar': 6418,\n  'hand': 6414,\n  'planning': 6406,\n  'constraints': 6396,\n  'shot': 6395,\n  'generalization': 6374,\n  'improves': 6366,\n  'instance': 6356,\n  'uses': 6330,\n  'memory': 6317,\n  'stage': 6304,\n  'related': 6298,\n  'distance': 6288,\n  'videos': 6282,\n  'possible': 6232,\n  'robustness': 6195,\n  'functions': 6186,\n  'challenges': 6185,\n  'open': 6183,\n  'potential': 6170,\n  'unsupervised': 6169,\n  'consider': 6165,\n  'user': 6156,\n  'noise': 6136,\n  'thus': 6092,\n  'obtained': 6059,\n  'regions': 6054,\n  'sets': 6027,\n  'understanding': 6027,\n  'common': 6021,\n  'setting': 6021,\n  'need': 6014,\n  'among': 6013,\n  'objective': 5984,\n  'solve': 5974,\n  'output': 5969,\n  'capture': 5954,\n  'languages': 5949,\n  'mechanism': 5927,\n  'solutions': 5922,\n  'requires': 5893,\n  'develop': 5892,\n  'synthetic': 5865,\n  'reconstruction': 5853,\n  'properties': 5830,\n  'directly': 5827,\n  'maps': 5822,\n  'robots': 5811,\n  'decision': 5801,\n  'baseline': 5780,\n  'Experimental': 5758,\n  'similarity': 5721,\n  'competitive': 5717,\n  'goal': 5711,\n  'types': 5703,\n  'part': 5692,\n  'studies': 5687,\n  'Experiments': 5676,\n  'main': 5665,\n  'words': 5618,\n  'effectively': 5600,\n  'application': 5597,\n  'improvement': 5585,\n  'future': 5581,\n  'require': 5572,\n  'embedding': 5538,\n  'question': 5527,\n  'improved': 5449,\n  'reduce': 5447,\n  'frame': 5437,\n  'Neural': 5428,\n  'developed': 5412,\n  'form': 5409,\n  'original': 5358,\n  'labeled': 5354,\n  'layers': 5338,\n  'much': 5331,\n  'sparse': 5323,\n  'presents': 5319,\n  'localization': 5316,\n  'making': 5301,\n  'environments': 5287,\n  'lower': 5283,\n  'predict': 5274,\n  'pixel': 5273,\n  'sampling': 5255,\n  'aims': 5249,\n  'highly': 5224,\n  'higher': 5223,\n  'edge': 5222,\n  'scheme': 5217,\n  'extraction': 5196,\n  'selection': 5187,\n  'architectures': 5185,\n  'examples': 5182,\n  'latent': 5175,\n  'sentence': 5170,\n  'less': 5167,\n  'pairs': 5159,\n  'could': 5126,\n  'additional': 5111,\n  'ground': 5108,\n  'reasoning': 5086,\n  'structures': 5085,\n  'query': 5071,\n  'speed': 5047,\n  'fusion': 5047,\n  'flow': 5028,\n  'extensive': 5024,\n  'embeddings': 5023,\n  'shows': 5013,\n  'frames': 4999,\n  'tree': 4982,\n  'Network': 4973,\n  'metrics': 4966,\n  'clustering': 4963,\n  'person': 4955,\n  'issue': 4916,\n  'strong': 4900,\n  'popular': 4879,\n  'agent': 4875,\n  'instances': 4872,\n  'good': 4850,\n  'metric': 4841,\n  'especially': 4835,\n  'encoder': 4831,\n  'aware': 4799,\n  'automatic': 4798,\n  'widely': 4798,\n  'explore': 4792,\n  'content': 4783,\n  'cases': 4756,\n  'difficult': 4744,\n  'conditions': 4736,\n  'policy': 4736,\n  'traditional': 4735,\n  'random': 4733,\n  'automatically': 4733,\n  'identify': 4728,\n  'recently': 4720,\n  'second': 4717,\n  'joint': 4712,\n  'probability': 4707,\n  'efficiently': 4707,\n  'predictions': 4705,\n  'respectively': 4704,\n  'annotated': 4672,\n  'free': 4670,\n  'annotations': 4670,\n  'resulting': 4654,\n  'adaptation': 4650,\n  'sample': 4647,\n  'corresponding': 4638,\n  'computation': 4637,\n  'score': 4633,\n  'dimensional': 4621,\n  'facial': 4610,\n  'diverse': 4605,\n  'years': 4575,\n  'investigate': 4555,\n  'English': 4552,\n  'made': 4544,\n  'bound': 4541,\n  'enables': 4522,\n  'usually': 4510,\n  'patterns': 4509,\n  'uncertainty': 4503,\n  'Learning': 4479,\n  'rate': 4476,\n  'promising': 4471,\n  'extract': 4464,\n  'path': 4454,\n  'fast': 4454,\n  'compare': 4452,\n  'value': 4444,\n  'classifier': 4440,\n  'practical': 4435,\n  'retrieval': 4415,\n  'evaluated': 4410,\n  'average': 4405,\n  'adaptive': 4403,\n  'presented': 4398,\n  'development': 4394,\n  'autonomous': 4376,\n  'generative': 4375,\n  'term': 4374,\n  'yet': 4373,\n  'four': 4371,\n  'scenes': 4370,\n  'hard': 4366,\n  'learns': 4356,\n  'improvements': 4298,\n  'settings': 4297,\n  'identification': 4287,\n  'sub': 4280,\n  'manner': 4276,\n  'estimate': 4274,\n  'makes': 4273,\n  'parameter': 4269,\n  'either': 4268,\n  'matrix': 4257,\n  'measure': 4255,\n  'support': 4252,\n  'help': 4249,\n  'medical': 4246,\n  'times': 4230,\n  'apply': 4219,\n  'variety': 4217,\n  'introduced': 4216,\n  'interest': 4210,\n  'gradient': 4210,\n  'tuning': 4204,\n  'success': 4200,\n  'region': 4191,\n  'dense': 4189,\n  'full': 4186,\n  'series': 4186,\n  'supervision': 4168,\n  'gap': 4131,\n  'components': 4127,\n  'RGB': 4119,\n  'behavior': 4114,\n  'consists': 4107,\n  'named': 4097,\n  'weights': 4084,\n  'body': 4080,\n  'publicly': 4055,\n  'Transformer': 4047,\n  'relevant': 4046,\n  'corpus': 4040,\n  'strategies': 4032,\n  'style': 4008,\n  'simultaneously': 3994,\n  'detect': 3983,\n  'lack': 3977,\n  'appearance': 3970,\n  'annotation': 3968,\n  'Deep': 3957,\n  'public': 3948,\n  'produce': 3932,\n  'cloud': 3929,\n  'discriminative': 3927,\n  'actions': 3924,\n  'useful': 3919,\n  'individual': 3917,\n  'interaction': 3915,\n  'regression': 3913,\n  'changes': 3911,\n  'First': 3910,\n  'improving': 3907,\n  'proposes': 3889,\n  'wide': 3883,\n  'vector': 3874,\n  'critical': 3871,\n  'amount': 3865,\n  'baselines': 3847,\n  'example': 3844,\n  'background': 3839,\n  'resource': 3830,\n  'generating': 3819,\n  'agents': 3809,\n  'parallel': 3809,\n  'build': 3805,\n  'relations': 3804,\n  'consistency': 3799,\n  'values': 3798,\n  'contrast': 3789,\n  'CNNs': 3779,\n  'wise': 3777,\n  'alignment': 3759,\n  'dynamics': 3758,\n  'geometric': 3755,\n  'superior': 3753,\n  'analyze': 3738,\n  'sequences': 3734,\n  'type': 3733,\n  'independent': 3723,\n  'give': 3717,\n  'grained': 3716,\n  'power': 3715,\n  'augmentation': 3706,\n  'fixed': 3703,\n  'comparison': 3702,\n  'issues': 3699,\n  'parts': 3699,\n  'mapping': 3696,\n  'computing': 3683,\n  'describe': 3675,\n  'literature': 3669,\n  'epsilon': 3636,\n  'trajectory': 3634,\n  'top': 3634,\n  'categories': 3627,\n  'users': 3627,\n  'document': 3626,\n  'whether': 3624,\n  'leads': 3621,\n  'performed': 3616,\n  'driving': 3607,\n  'consistent': 3606,\n  'event': 3602,\n  'social': 3598,\n  'performs': 3597,\n  'decoder': 3589,\n  'found': 3582,\n  'unseen': 3579,\n  'required': 3578,\n  'hierarchical': 3562,\n  'semi': 3557,\n  'solving': 3551,\n  'relation': 3550,\n  'bias': 3549,\n  'along': 3548,\n  'sensor': 3547,\n  'NLP': 3542,\n  'rely': 3533,\n  'simulation': 3532,\n  'length': 3523,\n  'Networks': 3523,\n  'since': 3518,\n  'per': 3511,\n  'great': 3501,\n  'weight': 3500,\n  'continuous': 3495,\n  'extracted': 3495,\n  'faster': 3495,\n  'theory': 3488,\n  'area': 3485,\n  'Based': 3481,\n  'defined': 3469,\n  'truth': 3466,\n  'sentences': 3463,\n  'jointly': 3441,\n  'noisy': 3439,\n  'color': 3439,\n  'towards': 3436,\n  'transformer': 3431,\n  'pipeline': 3424,\n  'provided': 3415,\n  'leverage': 3409,\n  'binary': 3401,\n  'maximum': 3400,\n  'conducted': 3391,\n  'combination': 3390,\n  'nodes': 3383,\n  'increasing': 3381,\n  'Recent': 3373,\n  'distributions': 3369,\n  'demonstrated': 3365,\n  'increase': 3364,\n  'modal': 3360,\n  'box': 3356,\n  'zero': 3355,\n  'speech': 3353,\n  'prove': 3351,\n  'importance': 3342,\n  'surface': 3339,\n  'constraint': 3335,\n  'exploit': 3334,\n  'take': 3334,\n  'tackle': 3333,\n  'edges': 3332,\n  'however': 3329,\n  'role': 3312,\n  'conduct': 3305,\n  'realistic': 3298,\n  'cannot': 3294,\n  'theoretical': 3288,\n  'concept': 3286,\n  'reinforcement': 3280,\n  'complete': 3278,\n  'driven': 3272,\n  'crucial': 3272,\n  'factor': 3264,\n  'robotic': 3258,\n  'Therefore': 3258,\n  'represent': 3257,\n  'vehicle': 3255,\n  'light': 3238,\n  'change': 3238,\n  'bounds': 3232,\n  'considered': 3227,\n  'essential': 3227,\n  'implementation': 3222,\n  'typically': 3221,\n  'handle': 3220,\n  'interactions': 3217,\n  'semantics': 3210,\n  'studied': 3194,\n  'progress': 3194,\n  'become': 3191,\n  'queries': 3187,\n  'factors': 3187,\n  'testing': 3186,\n  'pseudo': 3181,\n  'validate': 3179,\n  'attributes': 3179,\n  'Existing': 3167,\n  'building': 3162,\n  'probabilistic': 3160,\n  'Although': 3152,\n  'easily': 3151,\n  'effect': 3142,\n  'impact': 3139,\n  'unlabeled': 3138,\n  'Using': 3137,\n  'achieving': 3137,\n  'poses': 3136,\n  'comparable': 3131,\n  'questions': 3127,\n  'final': 3123,\n  'relative': 3123,\n  'namely': 3121,\n  'certain': 3117,\n  'conventional': 3113,\n  'contrastive': 3112,\n  'enhance': 3110,\n  'downstream': 3109,\n  'construct': 3107,\n  'energy': 3098,\n  'short': 3096,\n  'levels': 3095,\n  'procedure': 3093,\n  'constant': 3091,\n  'node': 3089,\n  'geometry': 3087,\n  'shared': 3075,\n  'answer': 3072,\n  'negative': 3071,\n  'underlying': 3064,\n  'explicitly': 3058,\n  'position': 3057,\n  'humans': 3057,\n  'unknown': 3056,\n  'fundamental': 3050,\n  'positive': 3050,\n  'sensors': 3049,\n  'convolution': 3043,\n  'errors': 3041,\n  'variables': 3041,\n  'weighted': 3037,\n  'contains': 3035,\n  'finding': 3030,\n  'dialogue': 3028,\n  'characteristics': 3027,\n  'Recently': 3027,\n  'approximate': 3025,\n  'reference': 3023,\n  'operations': 3002,\n  'respect': 3001,\n  'performing': 2994,\n  'outperform': 2984,\n  'component': 2984,\n  'aim': 2981,\n  'capable': 2979,\n  'tool': 2967,\n  'inputs': 2963,\n  'views': 2961,\n  'enable': 2959,\n  'major': 2957,\n  'compute': 2955,\n  'instead': 2948,\n  'people': 2947,\n  'BERT': 2947,\n  'precision': 2937,\n  'every': 2935,\n  'group': 2933,\n  'ones': 2928,\n  'structured': 2925,\n  'lead': 2921,\n  'diversity': 2918,\n  'overall': 2917,\n  'rich': 2895,\n  'remains': 2894,\n  'entity': 2889,\n  'scores': 2888,\n  'modules': 2887,\n  'pattern': 2883,\n  'resources': 2874,\n  'base': 2870,\n  'convergence': 2852,\n  'manipulation': 2850,\n  'Convolutional': 2849,\n  'idea': 2847,\n  'reduction': 2844,\n  'providing': 2843,\n  'ratio': 2842,\n  'correlation': 2840,\n  'classifiers': 2839,\n  'arbitrary': 2828,\n  'texture': 2824,\n  'comprehensive': 2822,\n  'detector': 2816,\n  'empirical': 2809,\n  'suitable': 2804,\n  'variations': 2799,\n  'paradigm': 2798,\n  'Net': 2795,\n  'observed': 2789,\n  'allow': 2786,\n  'advantage': 2778,\n  'initial': 2777,\n  'combined': 2776,\n  'according': 2770,\n  'associated': 2766,\n  'location': 2762,\n  'mobile': 2762,\n  'pair': 2761,\n  'version': 2760,\n  'relationship': 2759,\n  'previously': 2753,\n  'perception': 2753,\n  'expensive': 2750,\n  'physical': 2747,\n  'means': 2746,\n  'whole': 2739,\n  'labeling': 2739,\n  'polynomial': 2738,\n  'utilize': 2732,\n  'encoding': 2729,\n  'practice': 2725,\n  'collected': 2722,\n  'perspective': 2722,\n  'limitations': 2720,\n  'total': 2719,\n  'takes': 2719,\n  'cameras': 2719,\n  'rules': 2718,\n  'tested': 2718,\n  'detectors': 2714,\n  'community': 2712,\n  'steps': 2709,\n  'fields': 2706,\n  'Given': 2704,\n  'must': 2702,\n  'contextual': 2697,\n  'trajectories': 2686,\n  'kernel': 2682,\n  'sensing': 2681,\n  'line': 2680,\n  'classical': 2679,\n  'synthesis': 2675,\n  'areas': 2674,\n  'structural': 2672,\n  'discuss': 2665,\n  'Despite': 2663,\n  'automated': 2661,\n  'following': 2659,\n  'filter': 2656,\n  'communication': 2655,\n  'events': 2653,\n  'powerful': 2651,\n  'extend': 2649,\n  'update': 2649,\n  'would': 2639,\n  'Code': 2639,\n  'GAN': 2637,\n  'traffic': 2636,\n  'nature': 2627,\n  'unified': 2626,\n  'degree': 2623,\n  'rank': 2622,\n  'navigation': 2620,\n  'measures': 2618,\n  'statistical': 2615,\n  'relationships': 2605,\n  'distributed': 2604,\n  'mean': 2597,\n  'leading': 2590,\n  'create': 2589,\n  'exploration': 2583,\n  'category': 2576,\n  'Multi': 2572,\n  'capability': 2568,\n  'mainly': 2568,\n  'regularization': 2567,\n  'together': 2562,\n  'signal': 2557,\n  'concepts': 2556,\n  'Additionally': 2554,\n  'database': 2553,\n  'advantages': 2550,\n  'upon': 2544,\n  'Second': 2531,\n  'expected': 2528,\n  'vertices': 2527,\n  'topic': 2524,\n  'alternative': 2521,\n  'accurately': 2514,\n  'combining': 2513,\n  'tools': 2513,\n  'compression': 2508,\n  'connected': 2504,\n  'whose': 2502,\n  'vertex': 2502,\n  'aspects': 2500,\n  'details': 2497,\n  'Thus': 2496,\n  'pixels': 2496,\n  'modalities': 2492,\n  'frequency': 2491,\n  'commonly': 2491,\n  'vectors': 2490,\n  'head': 2489,\n  'inter': 2489,\n  'linguistic': 2488,\n  'minimum': 2488,\n  'Gaussian': 2483,\n  'entities': 2482,\n  'stochastic': 2480,\n  'successfully': 2479,\n  'applying': 2471,\n  'invariant': 2469,\n  'active': 2465,\n  'imaging': 2465,\n  'meta': 2464,\n  'conditional': 2461,\n  'another': 2460,\n  'collection': 2460,\n  'far': 2460,\n  'ImageNet': 2459,\n  'transformation': 2458,\n  'least': 2454,\n  'subset': 2452,\n  'recurrent': 2450,\n  'formulation': 2450,\n  'parsing': 2450,\n  'introduces': 2449,\n  'implemented': 2447,\n  'bounding': 2446,\n  'attacks': 2446,\n  'predicting': 2445,\n  'trees': 2445,\n  'built': 2437,\n  'effects': 2431,\n  'running': 2426,\n  'larger': 2425,\n  'showing': 2422,\n  'devices': 2422,\n  'understand': 2421,\n  'constrained': 2418,\n  'guided': 2415,\n  'ensemble': 2413,\n  'expression': 2413,\n  'Bayesian': 2410,\n  'fact': 2409,\n  'account': 2406,\n  'vehicles': 2403,\n  'clinical': 2400,\n  'optical': 2397,\n  'quantitative': 2396,\n  'phase': 2393,\n  'clouds': 2391,\n  'hybrid': 2388,\n  'evidence': 2387,\n  'sentiment': 2386,\n  'generates': 2385,\n  'static': 2382,\n  'generalize': 2382,\n  'modality': 2382,\n  'texts': 2380,\n  'artificial': 2378,\n  'overcome': 2378,\n  'considering': 2378,\n  'partial': 2375,\n  'detecting': 2373,\n  'scenario': 2372,\n  'registration': 2369,\n  'attribute': 2366,\n  'states': 2365,\n  'summarization': 2364,\n  'safety': 2355,\n  'soft': 2344,\n  'feedback': 2344,\n  'report': 2337,\n  'outputs': 2335,\n  'spaces': 2328,\n  'risk': 2319,\n  'benefits': 2317,\n  'observations': 2310,\n  'capabilities': 2310,\n  'rather': 2302,\n  'etc': 2296,\n  'researchers': 2295,\n  'operation': 2290,\n  'define': 2283,\n  'captured': 2281,\n  'suffer': 2278,\n  'facilitate': 2278,\n  'combines': 2274,\n  'game': 2273,\n  'signals': 2271,\n  'predicted': 2270,\n  'leveraging': 2270,\n  'adapt': 2259,\n  'reducing': 2259,\n  'forward': 2257,\n  'hardware': 2257,\n  'explicit': 2255,\n  'include': 2253,\n  'saliency': 2252,\n  'reliable': 2251,\n  'response': 2247,\n  'margin': 2245,\n  'block': 2244,\n  'LSTM': 2239,\n  'Since': 2236,\n  'evaluations': 2232,\n  'seen': 2231,\n  'estimates': 2230,\n  'varying': 2226,\n  'discrete': 2222,\n  'property': 2217,\n  'experiment': 2214,\n  'necessary': 2213,\n  'advances': 2210,\n  'distillation': 2209,\n  'mask': 2206,\n  'particularly': 2205,\n  'external': 2205,\n  'elements': 2190,\n  'reduces': 2190,\n  'processes': 2188,\n  'manual': 2179,\n  'character': 2177,\n  ...},\n 10800771)"},"metadata":{}}]},{"cell_type":"code","source":"total_num = 0\ncnt_dict_cv = {}\nfor l in tqdm(total_word_cv):\n    for word in l:\n        total_num += 1\n        if word not in cnt_dict_cv.keys():\n            cnt_dict_cv[word] = 1\n        else:\n            cnt_dict_cv[word] += 1\ncnt_dict_cv = dict(sorted(cnt_dict_cv.items(), key=lambda x: x[1], reverse=True))\ncnt_dict_cv, total_num","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:53:25.710383Z","iopub.execute_input":"2023-05-21T04:53:25.710839Z","iopub.status.idle":"2023-05-21T04:53:29.332798Z","shell.execute_reply.started":"2023-05-21T04:53:25.710801Z","shell.execute_reply":"2023-05-21T04:53:29.331527Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 45588/45588 [00:03<00:00, 12988.68it/s]\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"({'image': 46826,\n  'based': 35049,\n  'method': 33771,\n  'model': 33238,\n  'images': 33129,\n  'learning': 30946,\n  'methods': 30444,\n  'data': 30129,\n  'propose': 27981,\n  'proposed': 27891,\n  'performance': 25813,\n  'network': 25533,\n  'paper': 22827,\n  'state': 22825,\n  'training': 22634,\n  'art': 22069,\n  'results': 21884,\n  'features': 21537,\n  'object': 21445,\n  'detection': 21324,\n  'using': 19831,\n  'dataset': 19441,\n  'segmentation': 19058,\n  'approach': 18809,\n  'datasets': 18617,\n  'information': 18439,\n  'models': 18208,\n  'different': 17515,\n  'show': 17489,\n  'feature': 17160,\n  'novel': 17005,\n  'deep': 16421,\n  'task': 15499,\n  'problem': 15400,\n  'video': 15228,\n  'new': 14265,\n  'recognition': 14014,\n  'framework': 13434,\n  'high': 13152,\n  'also': 13085,\n  'classification': 12922,\n  'multi': 12872,\n  'accuracy': 12856,\n  'large': 12811,\n  'demonstrate': 12635,\n  'However': 12595,\n  'neural': 12533,\n  'domain': 12487,\n  'networks': 12389,\n  'used': 12380,\n  'tasks': 12119,\n  'real': 11898,\n  'experiments': 11791,\n  'visual': 11752,\n  'level': 11737,\n  'time': 11561,\n  'semantic': 11558,\n  'existing': 11426,\n  'work': 11318,\n  'end': 11040,\n  'human': 10442,\n  'attention': 10416,\n  'available': 10260,\n  'trained': 10047,\n  'scale': 9995,\n  'face': 9820,\n  'objects': 9814,\n  'use': 9718,\n  'supervised': 9582,\n  'representation': 9522,\n  'present': 9329,\n  'first': 9308,\n  'point': 9043,\n  'well': 8984,\n  'challenging': 8965,\n  'pose': 8867,\n  'approaches': 8845,\n  'single': 8561,\n  'loss': 8468,\n  'estimation': 8428,\n  'vision': 8407,\n  'scene': 8338,\n  'multiple': 8288,\n  'algorithm': 8267,\n  'set': 8232,\n  'depth': 8153,\n  'target': 8145,\n  'temporal': 8098,\n  'quality': 8090,\n  'input': 8077,\n  'local': 8028,\n  'CNN': 7951,\n  'convolutional': 7818,\n  'learn': 7654,\n  'space': 7564,\n  'achieves': 7485,\n  'better': 7358,\n  'spatial': 7227,\n  'class': 7171,\n  'motion': 7073,\n  'low': 7023,\n  'applications': 7011,\n  'introduce': 6973,\n  'outperforms': 6930,\n  'module': 6759,\n  'resolution': 6757,\n  'without': 6737,\n  'improve': 6711,\n  'architecture': 6656,\n  'achieve': 6632,\n  'https': 6596,\n  'three': 6563,\n  'due': 6478,\n  'various': 6430,\n  'view': 6354,\n  'text': 6302,\n  'self': 6223,\n  'github': 6219,\n  'representations': 6185,\n  'camera': 6184,\n  'system': 6175,\n  'analysis': 6101,\n  'videos': 6005,\n  'process': 6000,\n  'knowledge': 5937,\n  'labels': 5916,\n  'com': 5908,\n  'compared': 5887,\n  'benchmark': 5865,\n  'address': 5837,\n  'prediction': 5836,\n  'effective': 5815,\n  'samples': 5799,\n  'algorithms': 5768,\n  'many': 5700,\n  'action': 5690,\n  'tracking': 5612,\n  'number': 5608,\n  'recent': 5605,\n  'several': 5599,\n  'fine': 5562,\n  'shape': 5554,\n  'world': 5523,\n  'effectiveness': 5522,\n  'efficient': 5515,\n  'design': 5467,\n  'source': 5448,\n  'non': 5442,\n  'previous': 5433,\n  'significantly': 5411,\n  'computer': 5410,\n  'global': 5371,\n  'Extensive': 5315,\n  'structure': 5264,\n  'specific': 5206,\n  'generate': 5190,\n  'regions': 5180,\n  'reconstruction': 5117,\n  'pixel': 5106,\n  'pre': 5078,\n  'research': 5040,\n  'provide': 4995,\n  'robust': 4991,\n  'important': 4954,\n  'parameters': 4923,\n  'classes': 4910,\n  'techniques': 4845,\n  'cross': 4829,\n  'accurate': 4769,\n  'stage': 4762,\n  'Specifically': 4740,\n  'frame': 4730,\n  'maps': 4696,\n  'code': 4680,\n  'frames': 4674,\n  'including': 4668,\n  'study': 4646,\n  'distribution': 4624,\n  'train': 4608,\n  'matching': 4600,\n  'simple': 4573,\n  'adversarial': 4538,\n  'facial': 4537,\n  'learned': 4507,\n  'benchmarks': 4477,\n  'significant': 4450,\n  'map': 4430,\n  'person': 4428,\n  'instance': 4423,\n  'order': 4401,\n  'across': 4358,\n  'field': 4344,\n  'graph': 4296,\n  'context': 4284,\n  'processing': 4213,\n  'strategy': 4181,\n  'evaluation': 4180,\n  'small': 4179,\n  'evaluate': 4177,\n  'fully': 4153,\n  'current': 4131,\n  'label': 4127,\n  'often': 4107,\n  'generation': 4076,\n  'achieved': 4069,\n  'fusion': 4048,\n  'layer': 4019,\n  'perform': 4011,\n  'test': 3988,\n  'works': 3988,\n  'synthetic': 3962,\n  'function': 3948,\n  'scenes': 3938,\n  'generated': 3933,\n  'systems': 3916,\n  'given': 3888,\n  'via': 3888,\n  'problems': 3886,\n  'shot': 3873,\n  'hand': 3870,\n  'unsupervised': 3866,\n  'Moreover': 3863,\n  'prior': 3861,\n  'localization': 3856,\n  'transfer': 3842,\n  'limited': 3838,\n  'noise': 3804,\n  'RGB': 3791,\n  'appearance': 3771,\n  'complex': 3750,\n  'designed': 3726,\n  'inference': 3674,\n  'points': 3673,\n  'standard': 3666,\n  'cost': 3646,\n  'key': 3623,\n  'annotations': 3620,\n  'computational': 3601,\n  'still': 3600,\n  'long': 3592,\n  'layers': 3573,\n  'Network': 3546,\n  'Furthermore': 3542,\n  'addition': 3541,\n  'may': 3509,\n  'applied': 3507,\n  'best': 3503,\n  'region': 3502,\n  'ground': 3495,\n  'challenge': 3480,\n  'discriminative': 3476,\n  'experimental': 3445,\n  'called': 3444,\n  'directly': 3422,\n  'even': 3416,\n  'capture': 3403,\n  'focus': 3385,\n  'CNNs': 3380,\n  'optimization': 3376,\n  'Experimental': 3374,\n  'labeled': 3373,\n  'Experiments': 3363,\n  'effectively': 3358,\n  'background': 3342,\n  'range': 3331,\n  'obtain': 3288,\n  'flow': 3267,\n  'robustness': 3252,\n  'ability': 3240,\n  'terms': 3236,\n  'dense': 3217,\n  'part': 3216,\n  'able': 3205,\n  'cloud': 3156,\n  'make': 3148,\n  'improves': 3146,\n  'within': 3138,\n  'similar': 3136,\n  'Finally': 3132,\n  'supervision': 3121,\n  'thus': 3099,\n  'color': 3098,\n  'sparse': 3096,\n  'similarity': 3075,\n  'retrieval': 3073,\n  'way': 3063,\n  'mechanism': 3062,\n  'aware': 3056,\n  'technique': 3035,\n  'step': 3034,\n  'domains': 3028,\n  'aims': 3022,\n  'among': 3011,\n  'allows': 3003,\n  'wise': 3003,\n  'latent': 2995,\n  'scenarios': 2989,\n  'generalization': 2988,\n  'adaptation': 2975,\n  'dynamic': 2971,\n  'modeling': 2969,\n  'natural': 2948,\n  'architectures': 2947,\n  'size': 2944,\n  'solution': 2928,\n  'obtained': 2926,\n  'search': 2917,\n  'like': 2915,\n  'efficiency': 2909,\n  'baseline': 2902,\n  'identification': 2890,\n  'challenges': 2888,\n  'shown': 2877,\n  'encoder': 2873,\n  'body': 2869,\n  'extensive': 2869,\n  'extract': 2855,\n  'Neural': 2848,\n  'output': 2834,\n  'provides': 2807,\n  'improvement': 2802,\n  'scheme': 2786,\n  'predict': 2785,\n  'medical': 2767,\n  'particular': 2762,\n  'consistency': 2762,\n  'manner': 2750,\n  'higher': 2746,\n  'sequence': 2744,\n  'texture': 2737,\n  'reduce': 2735,\n  'content': 2720,\n  'original': 2717,\n  'result': 2716,\n  'understanding': 2716,\n  'respectively': 2715,\n  'potential': 2705,\n  'grained': 2703,\n  'language': 2702,\n  'common': 2698,\n  'geometric': 2697,\n  'annotated': 2683,\n  'learns': 2683,\n  'error': 2668,\n  'issue': 2662,\n  'publicly': 2658,\n  'generative': 2656,\n  'corresponding': 2653,\n  'categories': 2646,\n  'memory': 2646,\n  'style': 2646,\n  'truth': 2644,\n  'require': 2643,\n  'highly': 2642,\n  'speed': 2640,\n  'joint': 2629,\n  'uses': 2621,\n  'embedding': 2619,\n  'Net': 2616,\n  'requires': 2605,\n  'convolution': 2602,\n  'related': 2599,\n  'additional': 2588,\n  'detector': 2585,\n  'detectors': 2573,\n  'yet': 2572,\n  'modal': 2568,\n  'detect': 2565,\n  'parts': 2564,\n  'improved': 2556,\n  'usually': 2555,\n  'superior': 2547,\n  'public': 2530,\n  'find': 2526,\n  'distance': 2523,\n  'estimate': 2522,\n  'general': 2519,\n  'need': 2504,\n  'less': 2477,\n  'much': 2471,\n  'alignment': 2468,\n  'widely': 2457,\n  'extraction': 2456,\n  'classifier': 2444,\n  'popular': 2438,\n  'especially': 2436,\n  'diverse': 2425,\n  'promising': 2418,\n  'made': 2415,\n  'unseen': 2415,\n  'pseudo': 2414,\n  'views': 2410,\n  'geometry': 2409,\n  'consists': 2405,\n  'pixels': 2403,\n  'solve': 2401,\n  'conditions': 2389,\n  'known': 2388,\n  'could': 2388,\n  'studies': 2383,\n  'simultaneously': 2383,\n  'predictions': 2382,\n  'Deep': 2380,\n  'light': 2378,\n  'develop': 2369,\n  'future': 2366,\n  'poses': 2365,\n  'Convolutional': 2360,\n  'cameras': 2359,\n  'enables': 2357,\n  'competitive': 2357,\n  'Transformer': 2349,\n  'contrast': 2340,\n  'fast': 2338,\n  'traditional': 2334,\n  'realistic': 2326,\n  'metric': 2324,\n  'types': 2298,\n  'box': 2293,\n  'difficult': 2292,\n  'annotation': 2292,\n  'named': 2289,\n  'pairs': 2289,\n  'automatically': 2288,\n  'augmentation': 2282,\n  'years': 2281,\n  'transformer': 2279,\n  'surface': 2278,\n  'extracted': 2274,\n  'top': 2256,\n  'bounding': 2243,\n  'pipeline': 2241,\n  'evaluated': 2240,\n  'sampling': 2240,\n  'application': 2237,\n  'main': 2235,\n  'jointly': 2235,\n  'driving': 2234,\n  'structures': 2228,\n  'decoder': 2228,\n  'sub': 2216,\n  'linear': 2214,\n  'recently': 2212,\n  'second': 2212,\n  'registration': 2209,\n  'imaging': 2206,\n  'adaptive': 2205,\n  'GAN': 2205,\n  'ImageNet': 2199,\n  'Code': 2195,\n  'optical': 2191,\n  'automatic': 2190,\n  'full': 2189,\n  'regression': 2188,\n  'four': 2187,\n  'strong': 2184,\n  'complexity': 2180,\n  'query': 2176,\n  'explore': 2163,\n  'score': 2152,\n  'saliency': 2147,\n  'synthesis': 2146,\n  'clustering': 2144,\n  'good': 2143,\n  'produce': 2135,\n  'clouds': 2134,\n  'term': 2132,\n  'great': 2131,\n  'First': 2127,\n  'variations': 2125,\n  'shows': 2114,\n  'unlabeled': 2112,\n  'resulting': 2110,\n  'presents': 2108,\n  'changes': 2106,\n  'attributes': 2103,\n  'Networks': 2101,\n  'efficiently': 2094,\n  'computation': 2088,\n  'consistent': 2078,\n  'makes': 2074,\n  'sample': 2070,\n  'possible': 2069,\n  'lack': 2066,\n  'components': 2066,\n  'tackle': 2063,\n  'semi': 2056,\n  'exploit': 2054,\n  'open': 2052,\n  'examples': 2045,\n  'details': 2043,\n  'machine': 2041,\n  'improvements': 2040,\n  'sets': 2034,\n  'instances': 2030,\n  'patches': 2023,\n  'average': 2019,\n  'interest': 2013,\n  'mask': 2007,\n  'form': 2006,\n  'proposes': 2004,\n  'developed': 2003,\n  'free': 2000,\n  'sequences': 1996,\n  'per': 1996,\n  'category': 1993,\n  'setting': 1992,\n  'patterns': 1989,\n  'enhance': 1987,\n  'modality': 1985,\n  'gap': 1978,\n  'edge': 1965,\n  'faces': 1962,\n  'introduced': 1958,\n  'metrics': 1958,\n  'performs': 1950,\n  'contrastive': 1945,\n  'constraints': 1943,\n  'noisy': 1935,\n  'properties': 1934,\n  'head': 1922,\n  'user': 1918,\n  'making': 1917,\n  'either': 1913,\n  'weights': 1910,\n  'online': 1909,\n  'rely': 1897,\n  'modules': 1896,\n  'leverage': 1895,\n  'modalities': 1894,\n  'Existing': 1892,\n  'patch': 1884,\n  'expression': 1882,\n  'practical': 1872,\n  'success': 1871,\n  'apply': 1869,\n  'wide': 1868,\n  'rate': 1867,\n  'conducted': 1865,\n  'mapping': 1855,\n  'invariant': 1855,\n  'stereo': 1846,\n  'optimal': 1845,\n  'conventional': 1845,\n  'final': 1844,\n  'performed': 1843,\n  'identify': 1842,\n  'help': 1838,\n  'individual': 1833,\n  'dimensional': 1821,\n  'guided': 1812,\n  'Recent': 1807,\n  'captured': 1803,\n  'critical': 1802,\n  'correlation': 1797,\n  'Based': 1796,\n  'amount': 1789,\n  'hierarchical': 1788,\n  'generating': 1787,\n  'area': 1784,\n  'variety': 1781,\n  'along': 1775,\n  'occlusion': 1775,\n  'COCO': 1774,\n  'progress': 1767,\n  'database': 1766,\n  'inter': 1765,\n  'leads': 1759,\n  'comparable': 1752,\n  'Learning': 1750,\n  'issues': 1746,\n  'objective': 1742,\n  'Image': 1741,\n  'comparison': 1740,\n  'improving': 1740,\n  'autonomous': 1740,\n  'Therefore': 1739,\n  'translation': 1739,\n  'explicitly': 1739,\n  'handle': 1731,\n  'filter': 1726,\n  'identity': 1725,\n  'hard': 1717,\n  'demonstrated': 1716,\n  'reference': 1708,\n  'branch': 1707,\n  'since': 1700,\n  'interaction': 1697,\n  'solutions': 1696,\n  'validate': 1693,\n  'illumination': 1688,\n  'gradient': 1688,\n  'transformation': 1682,\n  'fields': 1681,\n  'coarse': 1681,\n  'functions': 1680,\n  'Recently': 1679,\n  'shapes': 1673,\n  'quantitative': 1673,\n  'essential': 1672,\n  'build': 1669,\n  'achieving': 1662,\n  'binary': 1659,\n  'attribute': 1657,\n  'conduct': 1651,\n  'location': 1648,\n  'people': 1646,\n  'contains': 1646,\n  'measure': 1645,\n  'cases': 1643,\n  'easily': 1635,\n  'whole': 1635,\n  'namely': 1633,\n  'utilize': 1632,\n  'weakly': 1632,\n  'selection': 1631,\n  'investigate': 1631,\n  'uncertainty': 1626,\n  'channel': 1625,\n  'relevant': 1622,\n  'regularization': 1617,\n  'consider': 1611,\n  'precision': 1609,\n  'Although': 1608,\n  'crucial': 1607,\n  'goal': 1606,\n  'event': 1605,\n  'unified': 1596,\n  'increasing': 1595,\n  'monocular': 1592,\n  'super': 1589,\n  'vector': 1589,\n  'testing': 1582,\n  'block': 1579,\n  'boundary': 1579,\n  'sensing': 1576,\n  'development': 1574,\n  'presented': 1573,\n  'support': 1572,\n  'settings': 1568,\n  'matrix': 1567,\n  'cues': 1560,\n  'scales': 1556,\n  'increase': 1554,\n  'case': 1554,\n  'compare': 1552,\n  'superiority': 1548,\n  'margin': 1545,\n  'tuning': 1544,\n  'backbone': 1544,\n  'actions': 1543,\n  'foreground': 1536,\n  'boxes': 1535,\n  'mean': 1532,\n  'useful': 1525,\n  'times': 1525,\n  'typically': 1522,\n  'perspective': 1521,\n  'characteristics': 1519,\n  'labeling': 1518,\n  'environment': 1517,\n  'mainly': 1517,\n  'group': 1511,\n  'interactions': 1510,\n  'strategies': 1509,\n  'salient': 1509,\n  'combination': 1507,\n  'descriptors': 1507,\n  'comprehensive': 1503,\n  'distillation': 1503,\n  'accurately': 1498,\n  'instead': 1494,\n  'levels': 1492,\n  'Multi': 1492,\n  'perception': 1492,\n  'attacks': 1488,\n  'relationship': 1487,\n  'ones': 1487,\n  'faster': 1479,\n  'intra': 1469,\n  'represent': 1468,\n  'however': 1466,\n  'expensive': 1464,\n  'component': 1462,\n  'importance': 1459,\n  'relative': 1458,\n  'Second': 1458,\n  'masks': 1454,\n  'segment': 1449,\n  'predicted': 1444,\n  'remains': 1442,\n  'normal': 1441,\n  'towards': 1439,\n  'vehicle': 1437,\n  'detecting': 1435,\n  'take': 1430,\n  'contextual': 1429,\n  'distributions': 1423,\n  'advantage': 1419,\n  'areas': 1415,\n  'control': 1414,\n  'zero': 1413,\n  'Besides': 1409,\n  'bias': 1406,\n  'cannot': 1405,\n  'construct': 1404,\n  'teacher': 1403,\n  'filters': 1402,\n  'proposal': 1401,\n  'procedure': 1400,\n  'leading': 1398,\n  'inputs': 1394,\n  'weight': 1390,\n  'rich': 1390,\n  'random': 1382,\n  'relationships': 1381,\n  'diversity': 1380,\n  'role': 1379,\n  'baselines': 1376,\n  'driven': 1375,\n  'paradigm': 1375,\n  'combined': 1374,\n  'frequency': 1374,\n  'provided': 1370,\n  'positive': 1370,\n  'change': 1370,\n  'attack': 1370,\n  'power': 1367,\n  'outperform': 1367,\n  'idea': 1366,\n  'powerful': 1364,\n  'become': 1362,\n  'analyze': 1361,\n  'major': 1360,\n  'required': 1360,\n  'example': 1357,\n  'downstream': 1355,\n  'building': 1354,\n  'proposals': 1353,\n  'takes': 1351,\n  'classifiers': 1348,\n  'pooling': 1344,\n  'signal': 1342,\n  'overcome': 1338,\n  'generator': 1338,\n  'position': 1337,\n  'effect': 1336,\n  'line': 1333,\n  'encoding': 1332,\n  'short': 1332,\n  'enable': 1332,\n  'capability': 1332,\n  'found': 1328,\n  'according': 1327,\n  'rank': 1327,\n  'suffer': 1320,\n  'advantages': 1318,\n  'collected': 1318,\n  'aim': 1318,\n  'capable': 1314,\n  'stream': 1313,\n  'denoising': 1312,\n  'sensor': 1312,\n  'spatio': 1309,\n  'rendering': 1307,\n  'estimated': 1307,\n  'kernel': 1306,\n  'sensors': 1305,\n  'overall': 1304,\n  'Despite': 1304,\n  'pair': 1303,\n  'factors': 1303,\n  'brain': 1303,\n  'performing': 1302,\n  'suitable': 1300,\n  'residual': 1300,\n  'spectral': 1299,\n  'student': 1293,\n  'correspondence': 1292,\n  'environments': 1292,\n  'relations': 1292,\n  'negative': 1289,\n  'compression': 1289,\n  'fixed': 1288,\n  'manual': 1286,\n  'volume': 1285,\n  'defined': 1281,\n  'limitations': 1280,\n  'lower': 1278,\n  'locations': 1275,\n  'LiDAR': 1275,\n  'steps': 1272,\n  'operations': 1271,\n  'scores': 1268,\n  'mAP': 1266,\n  'Gaussian': 1263,\n  'automated': 1259,\n  'fundamental': 1251,\n  'arbitrary': 1251,\n  'complementary': 1250,\n  'correspondences': 1250,\n  'together': 1249,\n  'combining': 1248,\n  'generates': 1247,\n  'type': 1241,\n  'Additionally': 1241,\n  'complete': 1237,\n  'parameter': 1234,\n  'preserving': 1234,\n  'lead': 1232,\n  'Compared': 1232,\n  'advances': 1232,\n  'tested': 1231,\n  'values': 1229,\n  'Using': 1224,\n  'commonly': 1221,\n  'reliable': 1218,\n  'leveraging': 1217,\n  'facilitate': 1216,\n  'seen': 1216,\n  'shared': 1216,\n  'density': 1215,\n  'unknown': 1213,\n  'Thus': 1210,\n  'literature': 1208,\n  'structural': 1207,\n  'etc': 1205,\n  'semantics': 1201,\n  'refinement': 1199,\n  'KITTI': 1199,\n  'codes': 1197,\n  'errors': 1196,\n  'devices': 1196,\n  'alleviate': 1195,\n  'qualitative': 1194,\n  'rotation': 1192,\n  'confidence': 1191,\n  'variation': 1190,\n  'transform': 1190,\n  'containing': 1188,\n  'mesh': 1185,\n  'boundaries': 1184,\n  'introduces': 1183,\n  'humans': 1182,\n  'initial': 1180,\n  'varying': 1177,\n  'estimates': 1174,\n  'priors': 1173,\n  'moving': 1173,\n  'considered': 1171,\n  'boost': 1171,\n  'independent': 1167,\n  'remote': 1167,\n  'employ': 1166,\n  'continuous': 1165,\n  'embeddings': 1162,\n  'constraint': 1161,\n  'predicting': 1161,\n  'detailed': 1159,\n  'Unlike': 1155,\n  'lightweight': 1154,\n  'phase': 1154,\n  'fashion': 1154,\n  'contain': 1151,\n  'observed': 1151,\n  'multimodal': 1150,\n  'pattern': 1147,\n  'question': 1146,\n  'shift': 1145,\n  'reasoning': 1144,\n  'caused': 1143,\n  'clinical': 1136,\n  'applying': 1136,\n  'underlying': 1136,\n  'outputs': 1136,\n  'community': 1135,\n  'missing': 1134,\n  'guide': 1134,\n  'diffusion': 1129,\n  'Visual': 1128,\n  'considering': 1127,\n  'generic': 1127,\n  'reduces': 1126,\n  'post': 1125,\n  'Since': 1124,\n  'describe': 1122,\n  'conditional': 1120,\n  'recognize': 1117,\n  'compact': 1117,\n  'guidance': 1115,\n  'direction': 1114,\n  'Given': 1111,\n  'introducing': 1108,\n  'performances': 1108,\n  'consuming': 1106,\n  'mobile': 1106,\n  'diagnosis': 1104,\n  'iterative': 1104,\n  'employed': 1101,\n  'partial': 1101,\n  'leverages': 1101,\n  'recurrent': 1100,\n  'produces': 1100,\n  'successfully': 1099,\n  'signals': 1099,\n  'pedestrian': 1099,\n  'imagery': 1097,\n  'formulation': 1097,\n  'forward': 1096,\n  'crowd': 1095,\n  'combines': 1094,\n  'activity': 1093,\n  'counting': 1087,\n  'implicit': 1084,\n  'entire': 1081,\n  'adopt': 1081,\n  'enhancement': 1080,\n  'aggregation': 1080,\n  'Attention': 1079,\n  'benefits': 1078,\n  'eye': 1078,\n  'generalize': 1078,\n  'built': 1076,\n  'exploiting': 1076,\n  'nature': 1074,\n  'impact': 1074,\n  'far': 1071,\n  'intermediate': 1071,\n  'combine': 1069,\n  'decision': 1069,\n  'auxiliary': 1069,\n  'providing': 1068,\n  'adaptively': 1068,\n  'specifically': 1068,\n  'another': 1066,\n  'road': 1066,\n  'editing': 1066,\n  'create': 1065,\n  'relation': 1064,\n  'upon': 1063,\n  'fail': 1061,\n  'structured': 1061,\n  'series': 1060,\n  'estimating': 1059,\n  'captioning': 1058,\n  'explicit': 1057,\n  'wild': 1056,\n  'ResNet': 1056,\n  'anomaly': 1052,\n  'active': 1051,\n  'consistently': 1051,\n  'vectors': 1050,\n  'energy': 1049,\n  'surveillance': 1049,\n  'manually': 1049,\n  'value': 1047,\n  'respect': 1046,\n  'skeleton': 1046,\n  'report': 1045,\n  'reducing': 1044,\n  'traffic': 1044,\n  'observation': 1036,\n  'weighted': 1033,\n  'associated': 1028,\n  'reduction': 1028,\n  'would': 1028,\n  'adapt': 1028,\n  'augmented': 1026,\n  'photo': 1026,\n  'reconstruct': 1025,\n  'events': 1024,\n  'verify': 1023,\n  'blur': 1022,\n  'integrated': 1021,\n  'extra': 1020,\n  'static': 1020,\n  'operation': 1019,\n  'certain': 1018,\n  'evaluations': 1017,\n  'probability': 1017,\n  'Object': 1016,\n  'easy': 1015,\n  'around': 1013,\n  'incorporate': 1012,\n  'studied': 1011,\n  'projection': 1008,\n  'differences': 1007,\n  'classify': 1007,\n  'Video': 1007,\n  'practice': 1006,\n  'dual': 1006,\n  'Adversarial': 1006,\n  'MRI': 1005,\n  'difference': 1004,\n  'concept': 1003,\n  'ensemble': 1002,\n  'effects': 999,\n  'subject': 998,\n  'released': 998,\n  'computed': 997,\n  ...},\n 5139709)"},"metadata":{}}]},{"cell_type":"code","source":"with open('/kaggle/working/cnt_dict_cv.txt', 'w') as f:\n    f.write(str(cnt_dict_cv))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:54:18.855842Z","iopub.execute_input":"2023-05-21T04:54:18.856398Z","iopub.status.idle":"2023-05-21T04:54:18.894941Z","shell.execute_reply.started":"2023-05-21T04:54:18.856353Z","shell.execute_reply":"2023-05-21T04:54:18.893794Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"total_num = 0\ntop_dict = {}\nprint(len(top_list))\nfor l in tqdm(top_list):\n    for word in l:\n        total_num += 1\n        if word not in top_dict.items():\n            top_dict[word] = 1\n        else:\n            top_dict[word] += 1\ntop_dict = dict(sorted(top_dict.items(), key=lambda x: x[1], reverse=True))\nwith open('/kaggle/working/top_dict.txt', 'w') as f:\n    f.write(str(top_dict))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T04:44:49.250754Z","iopub.execute_input":"2023-05-21T04:44:49.251092Z","iopub.status.idle":"2023-05-21T04:44:50.783795Z","shell.execute_reply.started":"2023-05-21T04:44:49.251062Z","shell.execute_reply":"2023-05-21T04:44:50.782812Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"103272\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 103272/103272 [00:01<00:00, 71584.41it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}